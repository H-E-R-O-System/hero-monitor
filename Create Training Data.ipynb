{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import math\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from Affective_Computing.getPipeData import get_pipe_data\n",
    "import cv2\n",
    "from pygame import Rect, Vector2\n",
    "import numpy as np\n",
    "from Affective_Computing.PointCloud import FaceCloud\n",
    "import TrainedInceptionResnetV2\n",
    "from scipy.io import savemat, loadmat\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T09:54:28.849354Z",
     "start_time": "2023-11-13T09:54:28.845508Z"
    }
   },
   "id": "775e084508b48207"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "\n",
    "emotions = [\"Anger\", \"Contempt\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\", \"Surprise\"]\n",
    "\n",
    "dataset_path = \"/Users/benhoskings/Documents/Emotion Recognition/Datasets/AffectNet/Data/train_set/matlab\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:31:20.613528Z",
     "start_time": "2023-11-13T00:31:20.609438Z"
    }
   },
   "id": "da554000ab999c51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load all models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "150c7e8a427d4a8a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1699835480.612474       1 face_landmarker_graph.cc:169] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "base_options = python.BaseOptions(model_asset_path='Affective_Computing/face_landmarker_v2_with_blendshapes.task')\n",
    "options = vision.FaceLandmarkerOptions(base_options=base_options, output_face_blendshapes=True,\n",
    "                                       output_facial_transformation_matrixes=True, num_faces=1, )\n",
    "detector = vision.FaceLandmarker.create_from_options(options)\n",
    "\n",
    "model = TrainedInceptionResnetV2.load_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:31:22.997719Z",
     "start_time": "2023-11-13T00:31:20.612Z"
    }
   },
   "id": "c1def49f993a3c87"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "im_path = \"Affective_Computing/Sample_Images/Neutral.png\"\n",
    "# get image as RGB array\n",
    "img_array = cv2.cvtColor(cv2.imread(im_path), cv2.COLOR_BGR2RGB)\n",
    "# get image as mediapipe image\n",
    "img_mp = mp.Image(data=img_array, image_format=mp.ImageFormat.SRGB)\n",
    "face_landmarks, _, _ = get_pipe_data(detector, img_mp)\n",
    "ref_face = FaceCloud(face_landmarks)\n",
    "ref_face.preprocess()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:31:23.021243Z",
     "start_time": "2023-11-13T00:31:22.998385Z"
    }
   },
   "id": "d89dbb3930546642"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_sample_ids(class_count=None, seed=None):\n",
    "    counts = [24882, 3750, 3803, 6378, 134414, 74874, 25459, 14090]\n",
    "    label_count = dict(zip(emotions, counts))\n",
    "    \n",
    "    if class_count:\n",
    "        class_count = min([class_count, min(label_count.values())])\n",
    "    else:\n",
    "        class_count = min(label_count.values())\n",
    "        \n",
    "    ids1 = np.empty((class_count, 0))\n",
    "    ids2 = np.empty((0, 1))\n",
    "    \n",
    "    for idx, emotion in enumerate(emotions):\n",
    "        file_count = label_count[emotion]\n",
    "        emIds = np.random.permutation(np.arange(file_count))[:class_count]\n",
    "        start_idx = sum(counts[:idx])\n",
    "        ids1 = np.append(ids1, np.expand_dims(emIds, axis=1), axis=1)\n",
    "        ids2 = np.append(ids2, start_idx + emIds)\n",
    "        \n",
    "    return ids1, ids2, label_count\n",
    "\n",
    "def read_image(emotion, id):\n",
    "    def num_string(num):\n",
    "        if num != 0:\n",
    "            return f\"00000{int(num)}\"[int(math.log10(num)):]\n",
    "        else:\n",
    "            return \"000000\"\n",
    "        \n",
    "    file_path = f\"{dataset_path}/{emotion}/{num_string(id)}.png\"\n",
    "    img = cv2.cvtColor(cv2.imread(file_path), cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "\n",
    "def segment_and_resize(img_array, landmarks, size=None):\n",
    "    px_locations_x = landmarks[:, 0] * img_array.shape[1]\n",
    "    px_locations_y = landmarks[:, 1] * img_array.shape[0]\n",
    "    \n",
    "    max_x, min_x = max(px_locations_x), min(px_locations_x)\n",
    "    max_y, min_y = max(px_locations_y), min(px_locations_y)\n",
    "    \n",
    "    # create bounding box of face and scale to adjust for full head region\n",
    "    scale = Vector2(1.8, 1.6)\n",
    "    bbox = np.asarray([min_x, min_y, max_x-min_x, max_y-min_y], dtype=np.int16)\n",
    "    face_rect = Rect(bbox).scale_by(scale.x, scale.y)\n",
    "    face_rect = face_rect.clip(Rect((0, 0), img_array.shape[:2]))\n",
    "    cropped_img = img_array[face_rect.top:face_rect.bottom, face_rect.left:face_rect.right]\n",
    "    \n",
    "    if size:\n",
    "        cropped_img = cv2.resize(cropped_img, size)\n",
    "        \n",
    "    return cropped_img\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:31:23.034390Z",
     "start_time": "2023-11-13T00:31:23.025385Z"
    }
   },
   "id": "f98b9139d1eb0f3b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(30000,)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotionIDs, imageIDs, count = get_sample_ids()\n",
    "# IDs are columns\n",
    "emotionIDs.shape\n",
    "# IDs are in a row vector\n",
    "imageIDs.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:31:23.034583Z",
     "start_time": "2023-11-13T00:31:23.027670Z"
    }
   },
   "id": "fdac183e836cf1fc"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3750it [01:22, 45.62it/s]\n",
      "3750it [01:21, 46.14it/s]\n",
      "3750it [01:22, 45.22it/s]\n",
      "3750it [01:25, 43.88it/s]\n",
      "3750it [01:31, 41.17it/s]\n",
      "3750it [01:31, 40.90it/s]\n",
      "3750it [01:32, 40.56it/s]\n",
      "3750it [01:37, 38.51it/s]\n"
     ]
    }
   ],
   "source": [
    "image_size = (299, 299)\n",
    "shape_data = np.empty((0, 49))\n",
    "delta_data = np.empty((0, 234))\n",
    "blend_data = np.empty((0, 52))\n",
    "net_data = np.empty((0, 8))\n",
    "labels = np.empty((0, 1))\n",
    "\n",
    "shape, delta, blend, net = True, True, True, False\n",
    "\n",
    "for em_idx, emotion in enumerate(emotions):\n",
    "    # create array to allocate image data - will predict on NN together to leverage \n",
    "    # GPU acceleration\n",
    "    emotion_array = np.empty((0, image_size[0], image_size[1], 3))\n",
    "    \n",
    "    for im_count, image_id in tqdm(enumerate(emotionIDs[:, em_idx])):\n",
    "        try:\n",
    "            # MxNx3 numpy array\n",
    "            img_array = read_image(emotion, image_id)\n",
    "            \n",
    "            # create mediapipe image object\n",
    "            img_mp = mp.Image(data=img_array, image_format=mp.ImageFormat.SRGB)\n",
    "            face_landmarks, blend_feature, _ = get_pipe_data(detector, img_mp)\n",
    "            \n",
    "            # create shape feature\n",
    "            face = FaceCloud(face_landmarks)\n",
    "            face.preprocess()\n",
    "            if shape:\n",
    "                shape_feature = face.create_shape_feature()\n",
    "                shape_data = np.append(shape_data, np.reshape(shape_feature, (1, -1)), axis=0)\n",
    "            if delta:\n",
    "                delta_feature = face.create_delta_feature(ref_face)\n",
    "                delta_data = np.append(delta_data, np.reshape(delta_feature, (1, -1)), axis=0)\n",
    "            if blend:\n",
    "                blend_data = np.append(blend_data, np.reshape(blend_feature, (1, -1)), axis=0)\n",
    "            if net:\n",
    "            # crop face and resize for input into neural network\n",
    "                img_array = segment_and_resize(img_array, face_landmarks, image_size)\n",
    "                emotion_array = np.append(emotion_array, np.expand_dims(img_array, axis=0), axis=0)\n",
    "  \n",
    "            labels = np.append(labels, em_idx)\n",
    "            \n",
    "            # perform prediction on images in batches of 64\n",
    "            if emotion_array.shape[0] == 128 and net:\n",
    "                emotion_net_data = model.predict(emotion_array, verbose=0)\n",
    "                net_data = np.append(net_data, emotion_net_data, axis=0)\n",
    "                emotion_array = np.empty((0, image_size[0], image_size[1], 3))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # catch the remaining data \n",
    "    if net:\n",
    "        emotion_net_data = model.predict(emotion_array)\n",
    "        net_data = np.append(net_data, emotion_net_data, axis=0)\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T09:38:17.348021Z",
     "start_time": "2023-11-13T09:26:32.862399Z"
    }
   },
   "id": "79f15e7cdf7349da"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29788, 49) (29788, 234) (29788, 52) (29788, 8)\n"
     ]
    }
   ],
   "source": [
    "train_data = loadmat(\"Training_Data.mat\")\n",
    "if not net:\n",
    "    net_data = train_data[\"net_data\"]\n",
    "if not shape:\n",
    "    shape_data = train_data[\"shape_data\"]\n",
    "if not delta:\n",
    "    delta_data = train_data[\"delta_data\"]\n",
    "if not blend:\n",
    "    blend_data = train_data[\"blend_data\"]\n",
    "\n",
    "train_data = {\"shape_data\": shape_data, \"delta_data\": delta_data, \"blend_data\": blend_data, \"net_data\": net_data, \"labels\": labels}\n",
    "print(shape_data.shape, delta_data.shape, blend_data.shape, net_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T09:55:08.372809Z",
     "start_time": "2023-11-13T09:55:08.359168Z"
    }
   },
   "id": "6d41a556c95b9e0f"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "savemat(\"Training_Data_2.mat\", train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T09:55:12.473685Z",
     "start_time": "2023-11-13T09:55:12.372836Z"
    }
   },
   "id": "8777c72288e3d131"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "34042573a551007f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
