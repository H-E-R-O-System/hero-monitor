{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-23T10:17:33.824121Z",
     "start_time": "2024-02-23T10:17:31.063103Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import config\n",
    "from imutils import paths\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from affective_computing.get_pipe_data import get_pipe_data\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "from affective_computing.point_cloud import FaceCloud\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benhoskings/Documents/Pycharm/Hero_Monitor/venv/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:394: UserWarning: Skipping variable loading for optimizer 'adam', because it has 354 variables whereas the saved optimizer has 28 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "random.seed(101)\n",
    "# load the ResNet50 network and initialize the label encoder\n",
    "print(\"[INFO] loading network...\")\n",
    "# model = ResNet50(weights=\"imagenet\", include_top=False)\n",
    "model = keras.models.load_model('data/AffectInceptionResNetV3.keras')\n",
    "# model = keras.models.load_model(\"data/checkpoints/checkpoint.keras\")\n",
    "le = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T10:17:34.938876Z",
     "start_time": "2024-02-23T10:17:33.824896Z"
    }
   },
   "id": "d0692cd5f711ec16"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"AffectNN\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"AffectNN\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape             \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ image_input (\u001B[38;5;33mInputLayer\u001B[0m)        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m3\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ rescale (\u001B[38;5;33mRescaling\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m3\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ resnet50v2 (\u001B[38;5;33mFunctional\u001B[0m)         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m2048\u001B[0m)        │ \u001B[38;5;34m23,564,800\u001B[0m │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ global_pool                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)              │          \u001B[38;5;34m0\u001B[0m │\n│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                           │            │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ feature_vector (\u001B[38;5;33mDense\u001B[0m)          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m)               │    \u001B[38;5;34m409,800\u001B[0m │\n└─────────────────────────────────┴───────────────────────────┴────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">    Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ image_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ rescale (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ resnet50v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">23,564,800</span> │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ global_pool                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                           │            │\n├─────────────────────────────────┼───────────────────────────┼────────────┤\n│ feature_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)               │    <span style=\"color: #00af00; text-decoration-color: #00af00\">409,800</span> │\n└─────────────────────────────────┴───────────────────────────┴────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m23,974,600\u001B[0m (91.46 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,974,600</span> (91.46 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m23,974,600\u001B[0m (91.46 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,974,600</span> (91.46 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_shape = (224, 224, 3)\n",
    "\n",
    "inputs = keras.Input(shape=image_shape, name=\"image_input\")\n",
    "x = model.get_layer(\"rescale\") (inputs)\n",
    "x = model.get_layer(\"resnet50v2\")(x)[0]\n",
    "x = model.get_layer(\"global_pool\")(x)\n",
    "x = model.get_layer(\"feature_vector\")(x)\n",
    "new_model = keras.Model(inputs, x, name=\"AffectNN\")\n",
    "new_model.trainable = False\n",
    "\n",
    "new_model.summary()\n",
    "output_size = np.prod(new_model.output.shape[1:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T10:17:34.950499Z",
     "start_time": "2024-02-23T10:17:34.943904Z"
    }
   },
   "id": "6dc9a68aa927366d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing 'train_set split'...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1842 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "004eaf2f6db4424ea8b8fd4a24ce4aca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing 'val_set split'...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/12 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "790e2a4c91b0447382253e2af3b023ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loop over the data splits\n",
    "for split in (config.TRAIN, config.VAL):\n",
    "\t# grab all image paths in the current split\n",
    "\tprint(\"[INFO] processing '{} split'...\".format(split))\n",
    "\tp = os.path.sep.join([config.BASE_PATH, split])\n",
    "\t\n",
    "\timagePaths = list(paths.list_images(p))\n",
    "\t# randomly shuffle the image paths and then extract the class\n",
    "\t# labels from the file paths\n",
    "\trandom.shuffle(imagePaths)\n",
    "\tlabels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "\t# if the label encoder is None, create it\n",
    "\tle = LabelEncoder()\n",
    "\tle.fit(labels)\n",
    "\t# open the output CSV file for writing\n",
    "\t\n",
    "\tcsvPath = os.path.sep.join([config.BASE_CSV_PATH, f\"{split}_data.csv\"])\n",
    "\tcsv = open(csvPath, \"w\")\n",
    "\tcsv.write(\",\".join([\"path\", \"class\"] + [f\"col_{idx}\" for idx in range(output_size)]))\n",
    "\tcsv.write(\"\\n\")\n",
    "\t\n",
    "    # loop over the images in batches\n",
    "\timage_idx = 0\n",
    "\tfor (b, i) in enumerate(tqdm(range(0, len(imagePaths), config.BATCH_SIZE))):\n",
    "\t\t# extract the batch of images and labels, then initialize the\n",
    "\t\t# list of actual images that will be passed through the network\n",
    "\t\t# for feature extraction\n",
    "\t\t# print(\"[INFO] processing batch {}/{}\".format(b + 1,\n",
    "\t\t# \tint(np.ceil(len(imagePaths) / float(config.BATCH_SIZE)))))\n",
    "\t\tbatchPaths = imagePaths[i:i + config.BATCH_SIZE]\n",
    "\t\tbatchLabels = le.transform(labels[i:i + config.BATCH_SIZE])\n",
    "\t\tbatchImages = []\n",
    "\t\t# loop over the images and labels in the current batch\n",
    "\t\tfor imagePath in batchPaths:\n",
    "\t\t\t# load the input image using the Keras helper utility\n",
    "\t\t\t# while ensuring the image is resized to 224x224 pixels\n",
    "\t\t\timage = load_img(imagePath, target_size=(224, 224))\n",
    "\t\t\timage = img_to_array(image)\n",
    "\t\t\t# preprocess the image by (1) expanding the dimensions and\n",
    "\t\t\t# (2) subtracting the mean RGB pixel intensity from the\n",
    "\t\t\t# ImageNet dataset\n",
    "\t\t\timage = np.expand_dims(image, axis=0)\n",
    "\t\t\timage = preprocess_input(image)\n",
    "\t\t\t# add the image to the batch\n",
    "\t\t\tbatchImages.append(image)\n",
    "        \n",
    "        # pass the images through the network and use the outputs as\n",
    "\t\t# our actual features, then reshape the features into a\n",
    "\t\t# flattened volume\n",
    "\t\tbatchImages = np.vstack(batchImages)\n",
    "\t\tfeatures = new_model.predict(batchImages, batch_size=config.BATCH_SIZE, verbose=0)\n",
    "\t\tfeatures = features.reshape((len(batchImages), output_size))\n",
    "\t\t\n",
    "\t\tfeatures = np.asarray(features, np.float16)\n",
    "\t\t# print(features.dtype)\n",
    "\n",
    "\t\t# loop over the class labels and extracted features\n",
    "\t\tfor idx, (label, vec) in enumerate(zip(batchLabels, features)):\n",
    "\t\t\t# construct a row that exists of the class label and\n",
    "\t\t\t# extracted features\n",
    "\t\t\tvec = \",\".join([str(v) for v in vec])\n",
    "\t\t\tcsv.write(f\"{batchPaths[idx]},{label},{vec}\\n\")\n",
    "\t# close the CSV file\n",
    "\tcsv.close()\n",
    "# serialize the label encoder to disk\n",
    "f = open(config.LE_PATH, \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T11:13:11.133228Z",
     "start_time": "2024-02-23T10:17:34.949980Z"
    }
   },
   "id": "8a84aab477461f5b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1708691409.775504       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2 Max\n",
      "W0000 00:00:1708691409.775808       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3aeb6e1f3a3348ca84406ab3965bfe99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benhoskings/Documents/Pycharm/Hero_Monitor/venv/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1495, 302)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
    "options = vision.FaceLandmarkerOptions(base_options=base_options, output_face_blendshapes=True,\n",
    "                                       output_facial_transformation_matrixes=True, num_faces=1, )\n",
    "detector = vision.FaceLandmarker.create_from_options(options)\n",
    "\n",
    "for data_set in (config.VAL, config.TRAIN):\n",
    "\t\n",
    "\timage_size = (224, 224)\n",
    "\tshape_data = np.empty((0, 49))\n",
    "\tblend_data = np.empty((0, 52))\n",
    "\tlabels = np.empty((0, 1))\n",
    "\tnet_data = pd.read_csv(f\"{data_set}_data.csv\")\n",
    "\t\n",
    "\tfeature_count = net_data.shape[1]-2\n",
    "\tnet_data.columns = [\"path\", \"class\"] + [f\"col_{idx}\" for idx in range(feature_count)]\n",
    "\tnet_data = net_data.set_index(\"path\")\n",
    "\tfail_paths = []\n",
    "\tfor im_path in tqdm(net_data.index):\n",
    "\t\timg_array = cv2.cvtColor(cv2.imread(im_path), cv2.COLOR_BGR2RGB)\n",
    "\t\t# get image as mediapipe image\n",
    "\t\timg_mp = mp.Image(data=img_array, image_format=mp.ImageFormat.SRGB)\n",
    "\t\tface_landmarks, blend_feature, _ = get_pipe_data(detector, img_mp)\n",
    "\t\tif face_landmarks is not None:\n",
    "\t\t\tface = FaceCloud(face_landmarks)\n",
    "\t\t\tface.preprocess()\n",
    "\t\t\tshape_feature = face.create_shape_feature()\n",
    "\t\t\tshape_data = np.append(shape_data, np.reshape(shape_feature, (1, -1)), axis=0)\n",
    "\t\t\tblend_data = np.append(blend_data, np.reshape(blend_feature, (1, -1)), axis=0)\n",
    "\t\telse:\n",
    "\t\t\tfail_paths.append(im_path)\n",
    "\t\n",
    "\tface_data = pd.DataFrame(data=np.concatenate([shape_data, blend_data], axis=1), \n",
    "\t\t\t\t\t\t\t columns=[f\"shape_{idx}\" for idx in range(shape_data.shape[1])]+[f\"blend_{idx}\" for idx in range(blend_data.shape[1])])\n",
    "\t\n",
    "\tnet_data_2 = net_data[np.logical_not(net_data.index.isin(fail_paths))]\n",
    "\tface_data = face_data.set_index(net_data_2.index)\n",
    "\t\n",
    "\tfuse_data = net_data_2.join(face_data)\n",
    "\tfuse_data.index = range(fuse_data.shape[0])\n",
    "\tfuse_data.to_csv(f\"fuse_data_{data_set}.csv\", index=False)\n",
    "\tprint(fuse_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:30:28.159400Z",
     "start_time": "2024-02-23T12:30:09.774569Z"
    }
   },
   "id": "12ba37c526df3c22"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228830, 302)\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:18:40.045947Z",
     "start_time": "2024-02-23T12:18:18.844674Z"
    }
   },
   "id": "e46a03d6943aa5f8"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T12:18:40.046564Z",
     "start_time": "2024-02-23T12:18:40.044449Z"
    }
   },
   "id": "866af42f9cbdaf92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
