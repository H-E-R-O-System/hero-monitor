{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-05T12:09:56.631842Z",
     "start_time": "2024-03-05T12:09:53.874117Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "import config\n",
    "from imutils import paths\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from affective_computing.get_pipe_data import get_pipe_data\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "from affective_computing.point_cloud import FaceCloud\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benhoskings/Documents/Pycharm/Hero_Monitor/venv2/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 354 variables whereas the saved optimizer has 28 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "random.seed(101)\n",
    "# load the ResNet50 network and initialize the label encoder\n",
    "print(\"[INFO] loading network...\")\n",
    "model = keras.models.load_model('../models/AffectInceptionResNetV3.keras')\n",
    "# model = keras.models.load_model(\"data/checkpoints/checkpoint.keras\")\n",
    "le = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T12:09:57.812151Z",
     "start_time": "2024-03-05T12:09:56.633180Z"
    }
   },
   "id": "d0692cd5f711ec16"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"AffectNN\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"AffectNN\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ image_input (\u001B[38;5;33mInputLayer\u001B[0m)        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m3\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ rescale (\u001B[38;5;33mRescaling\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m224\u001B[0m, \u001B[38;5;34m3\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ resnet50v2 (\u001B[38;5;33mFunctional\u001B[0m)         │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m2048\u001B[0m)     │    \u001B[38;5;34m23,564,800\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_pool                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ feature_vector (\u001B[38;5;33mDense\u001B[0m)          │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m)            │       \u001B[38;5;34m409,800\u001B[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ image_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ rescale (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ resnet50v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,564,800</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_pool                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ feature_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">409,800</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m23,974,600\u001B[0m (91.46 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,974,600</span> (91.46 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m23,974,600\u001B[0m (91.46 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,974,600</span> (91.46 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_shape = (224, 224, 3)\n",
    "\n",
    "inputs = keras.Input(shape=image_shape, name=\"image_input\")\n",
    "x = model.get_layer(\"rescale\") (inputs)\n",
    "x = model.get_layer(\"resnet50v2\")(x)[0]\n",
    "x = model.get_layer(\"global_pool\")(x)\n",
    "x = model.get_layer(\"feature_vector\")(x)\n",
    "new_model = keras.Model(inputs, x, name=\"AffectNN\")\n",
    "new_model.trainable = False\n",
    "\n",
    "new_model.summary()\n",
    "output_size = np.prod(new_model.output.shape[1:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T12:09:57.827215Z",
     "start_time": "2024-03-05T12:09:57.811982Z"
    }
   },
   "id": "6dc9a68aa927366d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing 'train_set split'...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1842 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1415e92a846146d9a86976219101c7a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loop over the data splits\n",
    "for split in (config.TRAIN, ):\n",
    "\t# grab all image paths in the current split\n",
    "\tprint(\"[INFO] processing '{} split'...\".format(split))\n",
    "\tp = os.path.sep.join([config.BASE_PATH, split])\n",
    "\t\n",
    "\timagePaths = list(paths.list_images(p))\n",
    "\t# randomly shuffle the image paths and then extract the class\n",
    "\t# labels from the file paths\n",
    "\trandom.shuffle(imagePaths)\n",
    "\tlabels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "\t# if the label encoder is None, create it\n",
    "\tle = LabelEncoder()\n",
    "\tle.fit(labels)\n",
    "\t# open the output CSV file for writing\n",
    "\t\n",
    "\tcsvPath = os.path.sep.join([config.BASE_CSV_PATH, f\"{split}_data.csv\"])\n",
    "\tcsv = open(csvPath, \"w\")\n",
    "\tcsv.write(\",\".join([\"path\", \"class\"] + [f\"col_{idx}\" for idx in range(output_size)]))\n",
    "\tcsv.write(\"\\n\")\n",
    "\t\n",
    "    # loop over the images in batches\n",
    "\timage_idx = 0\n",
    "\tfor (b, i) in enumerate(tqdm(range(0, len(imagePaths), config.BATCH_SIZE))):\n",
    "\t\t# extract the batch of images and labels, then initialize the\n",
    "\t\t# list of actual images that will be passed through the network\n",
    "\t\t# for feature extraction\n",
    "\t\tbatchPaths = imagePaths[i:i + config.BATCH_SIZE]\n",
    "\t\tbatchLabels = le.transform(labels[i:i + config.BATCH_SIZE])\n",
    "\t\tbatchImages = []\n",
    "\t\t# loop over the images and labels in the current batch\n",
    "\t\tfor imagePath in batchPaths:\n",
    "\t\t\t# load the input image using the Keras helper utility\n",
    "\t\t\t# while ensuring the image is resized to 224x224 pixels\n",
    "\t\t\timage = load_img(imagePath, target_size=(224, 224))\n",
    "\t\t\timage = img_to_array(image)\n",
    "\t\t\t# preprocess the image by (1) expanding the dimensions and\n",
    "\t\t\t# (2) subtracting the mean RGB pixel intensity from the\n",
    "\t\t\t# ImageNet dataset\n",
    "\t\t\timage = np.expand_dims(image, axis=0)\n",
    "\t\t\timage = preprocess_input(image)\n",
    "\t\t\t# add the image to the batch\n",
    "\t\t\tbatchImages.append(image)\n",
    "        \n",
    "        # pass the images through the network and use the outputs as\n",
    "\t\t# our actual features, then reshape the features into a\n",
    "\t\t# flattened volume\n",
    "\t\tbatchImages = np.vstack(batchImages)\n",
    "\t\tfeatures_1 = new_model.predict(batchImages, batch_size=config.BATCH_SIZE, verbose=0)\n",
    "\t\tfeatures_2 = model.predict(batchImages, batch_size=config.BATCH_SIZE, verbose=0)\n",
    "\t\t\n",
    "\t\tfeatures = np.concatenate([features_1, features_2], axis=1)\n",
    "\t\tfeatures = features.reshape((-1, output_size+3))\n",
    "\t\t\n",
    "\t\tfeatures = np.asarray(features, np.float16)\n",
    "\t\t# print(features.dtype)\n",
    "\n",
    "\t\t# loop over the class labels and extracted features\n",
    "\t\tfor idx, (label, vec) in enumerate(zip(batchLabels, features)):\n",
    "\t\t\t# construct a row that exists of the class label and\n",
    "\t\t\t# extracted features\n",
    "\t\t\tvec = \",\".join([str(v) for v in vec])\n",
    "\t\t\tcsv.write(f\"{batchPaths[idx]},{label},{vec}\\n\")\n",
    "\t\t\t\n",
    "\t# close the CSV file\n",
    "\tcsv.close()\n",
    "# serialize the label encoder to disk\n",
    "f = open(config.LE_PATH, \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-05T12:23:31.964344Z"
    }
   },
   "id": "8a84aab477461f5b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FieldDescriptor' object has no attribute '_default_constructor'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m base_options \u001B[38;5;241m=\u001B[39m python\u001B[38;5;241m.\u001B[39mBaseOptions(model_asset_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mface_landmarker_v2_with_blendshapes.task\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m options \u001B[38;5;241m=\u001B[39m vision\u001B[38;5;241m.\u001B[39mFaceLandmarkerOptions(base_options\u001B[38;5;241m=\u001B[39mbase_options, output_face_blendshapes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      3\u001B[0m                                        output_facial_transformation_matrixes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, num_faces\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, )\n\u001B[0;32m----> 4\u001B[0m detector \u001B[38;5;241m=\u001B[39m \u001B[43mvision\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFaceLandmarker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_from_options\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m base_options \u001B[38;5;241m=\u001B[39m python\u001B[38;5;241m.\u001B[39mBaseOptions(model_asset_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124membedder.tflite\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m options \u001B[38;5;241m=\u001B[39m vision\u001B[38;5;241m.\u001B[39mImageEmbedderOptions(base_options\u001B[38;5;241m=\u001B[39mbase_options, l2_normalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, quantize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Documents/Pycharm/Hero_Monitor/venv2/lib/python3.10/site-packages/mediapipe/tasks/python/vision/face_landmarker.py:3105\u001B[0m, in \u001B[0;36mFaceLandmarker.create_from_options\u001B[0;34m(cls, options)\u001B[0m\n\u001B[1;32m   3091\u001B[0m   output_streams\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m   3092\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([_FACE_GEOMETRY_TAG, _FACE_GEOMETRY_STREAM_NAME])\n\u001B[1;32m   3093\u001B[0m   )\n\u001B[1;32m   3095\u001B[0m task_info \u001B[38;5;241m=\u001B[39m _TaskInfo(\n\u001B[1;32m   3096\u001B[0m     task_graph\u001B[38;5;241m=\u001B[39m_TASK_GRAPH_NAME,\n\u001B[1;32m   3097\u001B[0m     input_streams\u001B[38;5;241m=\u001B[39m[\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3102\u001B[0m     task_options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   3103\u001B[0m )\n\u001B[1;32m   3104\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\n\u001B[0;32m-> 3105\u001B[0m     \u001B[43mtask_info\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_graph_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3106\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable_flow_limiting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mode\u001B[49m\n\u001B[1;32m   3107\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m_RunningMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLIVE_STREAM\u001B[49m\n\u001B[1;32m   3108\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   3109\u001B[0m     options\u001B[38;5;241m.\u001B[39mrunning_mode,\n\u001B[1;32m   3110\u001B[0m     packets_callback \u001B[38;5;28;01mif\u001B[39;00m options\u001B[38;5;241m.\u001B[39mresult_callback \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   3111\u001B[0m )\n",
      "File \u001B[0;32m~/Documents/Pycharm/Hero_Monitor/venv2/lib/python3.10/site-packages/mediapipe/tasks/python/core/task_info.py:97\u001B[0m, in \u001B[0;36mTaskInfo.generate_graph_config\u001B[0;34m(self, enable_flow_limiting)\u001B[0m\n\u001B[1;32m     93\u001B[0m   task_subgraph_options \u001B[38;5;241m=\u001B[39m calculator_options_pb2\u001B[38;5;241m.\u001B[39mCalculatorOptions()\n\u001B[1;32m     94\u001B[0m   task_subgraph_options\u001B[38;5;241m.\u001B[39mExtensions[task_options_proto\u001B[38;5;241m.\u001B[39mext]\u001B[38;5;241m.\u001B[39mCopyFrom(\n\u001B[1;32m     95\u001B[0m       task_options_proto\n\u001B[1;32m     96\u001B[0m   )\n\u001B[0;32m---> 97\u001B[0m   \u001B[43mnode_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCopyFrom\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask_subgraph_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     99\u001B[0m   \u001B[38;5;66;03m# Use the Any type for task_subgraph_options (proto3)\u001B[39;00m\n\u001B[1;32m    100\u001B[0m   task_subgraph_options \u001B[38;5;241m=\u001B[39m any_pb2\u001B[38;5;241m.\u001B[39mAny()\n",
      "File \u001B[0;32m~/Documents/Pycharm/Hero_Monitor/venv2/lib/python3.10/site-packages/google/protobuf/message.py:129\u001B[0m, in \u001B[0;36mMessage.CopyFrom\u001B[0;34m(self, other_msg)\u001B[0m\n\u001B[1;32m    127\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mClear()\n\u001B[0;32m--> 129\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMergeFrom\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother_msg\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Pycharm/Hero_Monitor/venv2/lib/python3.10/site-packages/google/protobuf/internal/python_message.py:1334\u001B[0m, in \u001B[0;36m_AddMergeFromMethod.<locals>.MergeFrom\u001B[0;34m(self, msg)\u001B[0m\n\u001B[1;32m   1331\u001B[0m field_value \u001B[38;5;241m=\u001B[39m fields\u001B[38;5;241m.\u001B[39mget(field)\n\u001B[1;32m   1332\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m field_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1333\u001B[0m   \u001B[38;5;66;03m# Construct a new object to represent this field.\u001B[39;00m\n\u001B[0;32m-> 1334\u001B[0m   field_value \u001B[38;5;241m=\u001B[39m \u001B[43mfield\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_default_constructor\u001B[49m(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1335\u001B[0m   fields[field] \u001B[38;5;241m=\u001B[39m field_value\n\u001B[1;32m   1336\u001B[0m field_value\u001B[38;5;241m.\u001B[39mMergeFrom(value)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'FieldDescriptor' object has no attribute '_default_constructor'"
     ]
    }
   ],
   "source": [
    "base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
    "options = vision.FaceLandmarkerOptions(base_options=base_options, output_face_blendshapes=True,\n",
    "                                       output_facial_transformation_matrixes=True, num_faces=1, )\n",
    "detector = vision.FaceLandmarker.create_from_options(options)\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='embedder.tflite')\n",
    "options = vision.ImageEmbedderOptions(base_options=base_options, l2_normalize=True, quantize=True)\n",
    "embedder = vision.ImageEmbedder.create_from_options(options)\n",
    "\n",
    "for data_set in (config.VAL, config.TRAIN):\n",
    "\t\n",
    "\timage_size = (224, 224)\n",
    "\tshape_data = np.empty((0, 49))\n",
    "\tblend_data = np.empty((0, 52))\n",
    "\tembedding_data = np.empty((0, 1024))\n",
    "\tlabels = np.empty((0, 1))\n",
    "\tnet_data = pd.read_csv(f\"training_data/{data_set}_data.csv\")\n",
    "\t\n",
    "\tfeature_count = net_data.shape[1]-2\n",
    "\tnet_data.columns = [\"path\", \"class\"] + [f\"col_{idx}\" for idx in range(feature_count)]\n",
    "\tnet_data = net_data.set_index(\"path\")\n",
    "\tfail_paths = []\n",
    "\tfor im_path in tqdm(net_data.index):\n",
    "\t\timg_array = cv2.cvtColor(cv2.imread(im_path), cv2.COLOR_BGR2RGB)\n",
    "\t\t# get image as mediapipe image\n",
    "\t\timg_mp = mp.Image(data=img_array, image_format=mp.ImageFormat.SRGB)\n",
    "\t\tembedding_result = embedder.embed(img_mp)\n",
    "\t\tface_landmarks, blend_feature, _ = get_pipe_data(detector, img_mp)\n",
    "\t\tif face_landmarks is not None:\n",
    "\t\t\tface = FaceCloud(face_landmarks)\n",
    "\t\t\tface.preprocess()\n",
    "\t\t\tshape_feature = face.create_shape_feature()\n",
    "\t\t\tshape_data = np.append(shape_data, np.reshape(shape_feature, (1, -1)), axis=0)\n",
    "\t\t\tblend_data = np.append(blend_data, np.reshape(blend_feature, (1, -1)), axis=0)\n",
    "\t\t\tembedding_data = np.append(embedding_data, embedding_result.embeddings[0].embedding.reshape(1, -1), axis=0)\n",
    "\t\telse:\n",
    "\t\t\tfail_paths.append(im_path)\n",
    "\t\n",
    "\tface_data = pd.DataFrame(\n",
    "\t\tdata=np.concatenate([shape_data, blend_data, embedding_data], axis=1), \n",
    "\t\tcolumns=([f\"shape_{idx}\" for idx in range(shape_data.shape[1])]+\n",
    "\t\t\t\t[f\"blend_{idx}\" for idx in range(blend_data.shape[1])]+\n",
    "\t\t\t\t[f\"embedding_{idx}\" for idx in range(embedding_data.shape[1])]))\n",
    "\t\n",
    "\tnet_data_2 = net_data[np.logical_not(net_data.index.isin(fail_paths))]\n",
    "\tface_data = face_data.set_index(net_data_2.index)\n",
    "\t\n",
    "\tfuse_data = net_data_2.join(face_data)\n",
    "\tfuse_data.index = range(fuse_data.shape[0])\n",
    "\tfuse_data.to_csv(f\"training_data/fuse_data_{data_set}.csv\", index=False)\n",
    "\tprint(fuse_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T12:09:58.382161Z",
     "start_time": "2024-03-05T12:09:57.910918Z"
    }
   },
   "id": "12ba37c526df3c22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T12:09:58.396321Z",
     "start_time": "2024-03-05T12:09:58.383330Z"
    }
   },
   "id": "e46a03d6943aa5f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-05T12:09:58.384669Z"
    }
   },
   "id": "866af42f9cbdaf92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
