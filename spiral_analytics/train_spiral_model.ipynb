{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "np.random.seed(100)\n",
    "os.chdir(\"/Users/benhoskings/Documents/Pycharm/Hero_Monitor/spiral_analytics\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# if np.arctan2(*np.flip(pos - self.center_offset)) > 0:\n",
    "#             angle = np.arctan2(*np.flip(pos - self.center_offset)) + 2 * np.pi * self.turns\n",
    "#         else:\n",
    "#             angle = np.arctan2(*np.flip(pos - self.center_offset)) + 2 * np.pi * (self.turns + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def drop_cols(df):\n",
    "    \n",
    "    return df.drop([\"z_pos\", \"pressure\", \"grip_angle\", \"test_id\"], axis=1)\n",
    "    \n",
    "def augment_data(input_data, spiral_radius, invert_y=True):\n",
    "    if invert_y:\n",
    "        data_aug = input_data.assign(\n",
    "            x_pos=(input_data[\"x_pos\"] - spiral_radius) / spiral_radius,\n",
    "            y_pos=(spiral_radius - input_data[\"y_pos\"]) / spiral_radius\n",
    "        )\n",
    "    else:\n",
    "        data_aug = input_data.assign(\n",
    "            x_pos=(input_data[\"x_pos\"] - spiral_radius) / spiral_radius,\n",
    "            y_pos=(input_data[\"y_pos\"] - spiral_radius) / spiral_radius\n",
    "        )\n",
    "        \n",
    "    data_aug = data_aug.assign(time = (data_aug[\"time\"] - min(data_aug[\"time\"]))/1000)\n",
    "    data_aug = data_aug.assign(magnitude=np.linalg.norm(data_aug[[\"x_pos\", \"y_pos\"]], axis=1))\n",
    "    \n",
    "    # plt.plot(data_aug[\"time\"], data_aug[\"theta\"])\n",
    "    # plt.show()\n",
    "\n",
    "    data_aug = data_aug.assign(distance = data_aug[\"magnitude\"].diff())\n",
    "    turn_count = 0\n",
    "    if data_aug.loc[0, \"y_pos\"] < 0:\n",
    "        turn_count -= 1\n",
    "    \n",
    "    turns = np.array([turn_count])\n",
    "    angles = np.array([])\n",
    "    for row_idx in data_aug.index:\n",
    "        pos = data_aug.loc[row_idx, [\"x_pos\", \"y_pos\"]].values\n",
    "        if row_idx > 0:\n",
    "            prev_pos = data_aug.loc[row_idx-1, [\"x_pos\", \"y_pos\"]].values\n",
    "            if pos[0] > 0 and prev_pos[0] > 0 and prev_pos[1] >= 0 > pos[1]:\n",
    "                turn_count -= 1  # anit-clockwise crossing of positive x-axis\n",
    "            elif pos[0] > 0 and prev_pos[0] > prev_pos[1] <= 0 < pos[1]:\n",
    "                turn_count += 1  # clockwise crossing of positive x-axis\n",
    "            turns = np.append(turns, turn_count)\n",
    "            \n",
    "        atan_result = np.arctan2(pos[1], pos[0])\n",
    "    \n",
    "        if atan_result > 0:\n",
    "            angle = atan_result + 2*np.pi*turn_count\n",
    "        else:\n",
    "            angle = atan_result + 2*np.pi*(turn_count+1)\n",
    "            \n",
    "        \n",
    "        angles = np.append(angles, angle)\n",
    "            \n",
    "    data_aug = data_aug.assign(\n",
    "        turns = turns,\n",
    "        theta = angles\n",
    "    )\n",
    "    \n",
    "    data_aug = data_aug.assign(\n",
    "        error=((data_aug[\"theta\"] / (2 * np.pi * 3)) - data_aug[\"magnitude\"]) * data_aug[\"theta\"],\n",
    "        angular_velocity = data_aug[\"theta\"].diff() / data_aug[\"time\"].diff()\n",
    "    )\n",
    "    return data_aug\n",
    "\n",
    "def create_feature(spiral_data):\n",
    "    spiral_data: pd.DataFrame\n",
    "    # spiral_data = spiral_data.copy().drop([\"x_pos\", \"y_pos\"], axis=1)\n",
    "    mean_values = np.mean(spiral_data, axis=0)\n",
    "    \n",
    "    # rms_vals = np.sqrt(np.mean(spiral_data**2, axis=0))\n",
    "    # max_values = np.max(spiral_data, axis=0)\n",
    "    \n",
    "    # # feature \n",
    "    # print(max_values[\"time\"])\n",
    "    \n",
    "    # feature is max time\n",
    "    # feature = np.array([max_values[\"time\"], mean_values[\"error\"], mean_values[\"angular_velocity\"]])\n",
    "    feature = np.array(np.concatenate([mean_values.values,]))\n",
    "    return feature"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "fe8e27f1fd98db4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = \"hw_dataset/control/C_0001.txt\"\n",
    "# X ; Y; Z; Pressure; GripAngle; Timestamp; Test ID\n",
    "test_data = pd.read_csv(file_path, sep=\";\", names=[\"x_pos\", \"y_pos\", \"z_pos\", \"pressure\", \"grip_angle\", \"time\", \"test_id\"])\n",
    "static_data = test_data.loc[test_data[\"test_id\"] == 0]\n",
    "dynamic_data = test_data.loc[test_data[\"test_id\"] == 1]\n",
    "\n",
    "static_data_aug = drop_cols(static_data)\n",
    "static_data_aug = augment_data(static_data_aug, spiral_radius=200)\n",
    "# print(static_data_aug.tail(), static_data_aug.columns)\n",
    "spiral_feature = create_feature(static_data_aug)\n",
    "print(spiral_feature.shape)\n",
    "print(spiral_feature.tail())\n",
    "\n",
    "plt.scatter(static_data_aug[\"x_pos\"], static_data_aug[\"y_pos\"])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(static_data_aug[\"time\"], static_data_aug[\"theta\"])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5f5b9a7a6244bf3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load spiral dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "356f74cdaa86fbae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "col_names = [\"x_pos\", \"y_pos\", \"z_pos\", \"pressure\", \"grip_angle\", \"time\", \"test_id\"]\n",
    "\n",
    "feature_df = pd.DataFrame(data=None, columns=([\"label\", \"set_id\"] + [f\"predictor_{idx}\" for idx in range(len(spiral_feature))]))\n",
    "\n",
    "base_path = \"hw_dataset\"\n",
    "times = []\n",
    "for dataset in (\"control\", \"parkinson\"):\n",
    "    file_names = os.listdir(f\"hw_dataset/{dataset}\")\n",
    "    file_paths = [os.path.join(base_path, dataset, file_name) for file_name in file_names if \".txt\" in file_name]\n",
    "    for idx, file_path in enumerate(file_paths):\n",
    "        test_data = pd.read_csv(file_path, sep=\";\", names=col_names)\n",
    "        static_data = test_data.loc[test_data[\"test_id\"] == 0]\n",
    "        if dataset == \"parkinson\":\n",
    "            times.append(max(static_data[\"time\"] - min(static_data[\"time\"])))\n",
    "        static_data_aug = drop_cols(static_data)\n",
    "        static_data_aug = augment_data(static_data_aug, spiral_radius=200)\n",
    "        spiral_feature = create_feature(static_data_aug)\n",
    "        if dataset == \"parkinson\":\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "            \n",
    "        feature_df.loc[len(feature_df)] = np.append([label, idx], spiral_feature)\n",
    "\n",
    "print(np.mean(times)/1000)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5323b01f13c1ab42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# sort into training / validation datasets "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4a945fa5b7bd398"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def partition_data(df, train_split, even_classes=True):\n",
    "\n",
    "    control_count = sum(df[\"label\"] == 0)\n",
    "    nd_count = sum(df[\"label\"] == 1)\n",
    "    if even_classes:\n",
    "        class_count = min(control_count, nd_count)\n",
    "        control_train_ids = np.random.permutation(range(control_count))[:int(class_count*train_split)]\n",
    "        nd_train_ids = np.random.permutation(range(nd_count))[:int(class_count*train_split)]\n",
    "    else:\n",
    "        control_train_ids = np.random.permutation(range(control_count))[:int(control_count*train_split)]\n",
    "        nd_train_ids = np.random.permutation(range(nd_count))[:int(nd_count*train_split)]\n",
    "    \n",
    "    control_val_ids = np.array([idx for idx in range(control_count) if not np.isin(idx, control_train_ids)])\n",
    "    nd_val_ids = np.array([idx for idx in range(nd_count) if not np.isin(idx, nd_train_ids)])\n",
    "    \n",
    "    train_df = df.loc[np.logical_or(\n",
    "        np.logical_and((df[\"label\"] == 0), np.isin(df[\"set_id\"], control_train_ids)), \n",
    "        np.logical_and((df[\"label\"] == 1), np.isin(df[\"set_id\"], nd_train_ids)))]\n",
    "    \n",
    "    val_df = df.loc[np.logical_or(\n",
    "        np.logical_and((df[\"label\"] == 0), np.isin(df[\"set_id\"], control_val_ids)), \n",
    "        np.logical_and((df[\"label\"] == 1), np.isin(df[\"set_id\"], nd_val_ids)))]\n",
    "    \n",
    "    return train_df, val_df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2a206167b1338ae0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conf_matrix_array = np.empty([2, 2, 0])\n",
    "print(conf_matrix_array.shape)\n",
    "\n",
    "print(conf_matrix_array.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f315dd5915bf8441"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "linear_accuracies = []\n",
    "mlp_accuracies = []\n",
    "svm_accuracies = []\n",
    "\n",
    "conf_matrix_array = np.empty([2, 2, 0])\n",
    "\n",
    "best_cost = 100\n",
    "best_model = None\n",
    "lr = LinearRegression()\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(11, 11), random_state=1, max_iter=int(5e6))\n",
    "# class_weight={0: 10}\n",
    "# svm_clf = svm.SVC(kernel=\"poly\",degree=2, class_weight={0:25, 1:15})\n",
    "svm_clf = svm.SVC(kernel=\"rbf\", class_weight={0:25, 1:15})\n",
    "\n",
    "for fold in range(500):\n",
    "    train_set, val_set = partition_data(feature_df, 0.8, even_classes=False)\n",
    "    \n",
    "    train_data = train_set.drop([\"label\", \"set_id\"], axis=1)\n",
    "    val_data = val_set.drop([\"label\", \"set_id\"], axis=1)\n",
    "    \n",
    "    # print(f\"control_count: {sum(train_set.label == 0)}, p count: {sum(train_set.label == 1)}\")\n",
    "    \n",
    "    # train_data = scaler.fit_transform(train_data)\n",
    "    # val_data = scaler.transform(val_data)\n",
    "    linear_model = lr.fit(train_data, train_set[\"label\"])\n",
    "    svm_clf.fit(train_data, train_set.label)\n",
    "    mlp.fit(scaler.fit_transform(train_data), train_set.label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mlp_acc = mlp.score(scaler.fit_transform(val_data), val_set.label)\n",
    "    mlp_accuracies.append(mlp_acc)\n",
    "    \n",
    "    svm_acc = svm_clf.score(val_data, val_set.label)\n",
    "    svm_accuracies.append(svm_acc)\n",
    "    \n",
    "    predictions = linear_model.predict(val_data)\n",
    "    classification = [val > 0.5 for val in predictions]\n",
    "    \n",
    "    cost = []\n",
    "    for idx, val in enumerate(predictions):\n",
    "        if val_set.label.values[idx] == 0:\n",
    "            cost.append(val)\n",
    "        else:\n",
    "            cost.append(-val)\n",
    "    #         \n",
    "    # print(linear_model.predict(val_data), val_set.label)\n",
    "    cm = confusion_matrix(classification, val_set.label)\n",
    "    conf_matrix_array = np.append(conf_matrix_array, np.expand_dims(cm, axis=2), axis=2)\n",
    "            \n",
    "    if np.linalg.norm(cost) < best_cost:\n",
    "        best_model = linear_model\n",
    "        best_cost = sum(cost)\n",
    "        \n",
    "    accuracy = (np.mean(np.equal(classification, val_set.label)))\n",
    "    linear_accuracies.append(accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7b69174fe3c3fc85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Mean regression accuracy: {np.mean(linear_accuracies)*100}%\")\n",
    "train_set, val_set = partition_data(feature_df, 0.8)\n",
    "val_data = val_set.drop([\"label\", \"set_id\"], axis=1)\n",
    "opt_predictions = best_model.predict(val_data)\n",
    "opt_classification = [val > 0.5 for val in opt_predictions]\n",
    "accuracy = (np.mean(np.equal(opt_classification, val_set.label)))\n",
    "cm = confusion_matrix(opt_classification, val_set.label)\n",
    "\n",
    "\n",
    "print(np.sum(conf_matrix_array, axis=2))\n",
    "\n",
    "df_cm = pd.DataFrame(np.sum(conf_matrix_array, axis=2), index = [\"control\", \"parkinons\"],\n",
    "                  columns = [\"control\", \"parkinons\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"control\", \"parkinsons\"])\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean mlp accuracy: {np.mean(mlp_accuracies)*100}%\")\n",
    "opt_predictions = mlp.predict(scaler.fit_transform(val_data))\n",
    "opt_classification = [val > 0.5 for val in opt_predictions]\n",
    "accuracy = (np.mean(np.equal(opt_classification, val_set.label)))\n",
    "print(f\"Best accuracy: {accuracy*100}%\")\n",
    "cm = confusion_matrix(opt_classification, val_set.label)\n",
    "print(cm)\n",
    "\n",
    "print(f\"Mean svm accuracy: {np.mean(svm_accuracies)*100}%\")\n",
    "opt_predictions = svm_clf.predict(val_data)\n",
    "opt_classification = [val > 0.5 for val in opt_predictions]\n",
    "accuracy = (np.mean(np.equal(opt_classification, val_set.label)))\n",
    "print(f\"Best accuracy: {accuracy*100}%\")\n",
    "cm = confusion_matrix(opt_classification, val_set.label)\n",
    "print(cm)\n",
    "\n",
    "print(train_set.shape, val_set.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "821414270537f73d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/benhoskings/Documents/Pycharm/Hero_Monitor\")\n",
    "joblib.dump(best_model, os.path.join(\"data/linear_regression_model.joblib\"))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "784738de1f042621"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train MLP\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b9f90fc73dd1b68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=2, shuffle=True)\n",
    "\n",
    "train_set, val_set = partition_data(feature_df, 0.8)\n",
    "train_data = train_set.drop([\"label\", \"set_id\"], axis=1)\n",
    "val_data = val_set.drop([\"label\", \"set_id\"], axis=1)\n",
    "\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.fit_transform(val_data)\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=int(1e5))\n",
    "\n",
    "X, y = feature_df.drop([\"label\", \"set_id\"], axis=1), feature_df[\"label\"]\n",
    "\n",
    "accuracies = []\n",
    "for train_indices, test_indices in kf.split(X):\n",
    "    clf.fit(X.loc[train_indices], y.loc[train_indices])\n",
    "    acc = clf.score(X.loc[test_indices], y.loc[test_indices])\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "print(f\"Average Accuracy: {np.mean(accuracies)}\")\n",
    "\n",
    "train_set, val_set = partition_data(feature_df, 0.8)\n",
    "val_data = val_set.drop([\"label\", \"set_id\"], axis=1)\n",
    "val_data = scaler.fit_transform(val_data)\n",
    "opt_predictions = clf.predict(val_data)\n",
    "opt_classification = [val > 0.5 for val in opt_predictions]\n",
    "accuracy = (np.mean(np.equal(opt_classification, val_set.label)))\n",
    "print(f\"Best accuracy: {accuracy*100}%\", best_cost)\n",
    "print(opt_predictions)\n",
    "\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(10, 10),\n",
    "#                     max_iter=50000, random_state=42)\n",
    "# \n",
    "# mlp.fit(train_data, train_set[\"label\"])\n",
    "#  \n",
    "# # Make predictions on the test data\n",
    "# y_pred = mlp.predict(val_data)\n",
    "# accuracy = accuracy_score(val_set.label, y_pred)\n",
    "# print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6e8027188504498f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2076ad5b793c3f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
